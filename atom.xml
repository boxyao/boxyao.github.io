<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://boxyao.github.io</id>
    <title>BoxYao</title>
    <updated>2020-04-29T10:59:52.959Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://boxyao.github.io"/>
    <link rel="self" href="https://boxyao.github.io/atom.xml"/>
    <subtitle>昨日已逝~</subtitle>
    <logo>https://boxyao.github.io/images/avatar.png</logo>
    <icon>https://boxyao.github.io/favicon.ico</icon>
    <rights>All rights reserved 2020, BoxYao</rights>
    <entry>
        <title type="html"><![CDATA[各种激活函数总结]]></title>
        <id>https://boxyao.github.io/post/ge-chong-ji-huo-han-shu-zong-jie/</id>
        <link href="https://boxyao.github.io/post/ge-chong-ji-huo-han-shu-zong-jie/">
        </link>
        <updated>2020-04-30T03:05:14.000Z</updated>
        <content type="html"><![CDATA[<h1 id="常用激活函数及其导数">常用激活函数及其导数</h1>
<table>
<thead>
<tr>
<th style="text-align:center">激活函数</th>
<th style="text-align:center">形式</th>
<th style="text-align:center">导数形式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Sigmod</td>
<td style="text-align:center">$$f\left( x \right)=\frac{1}{1+e^{-x}}$$</td>
<td style="text-align:center">$$f'\left( x \right)=f\left( x \right)\left( 1-f\left(x\right) \right)$$</td>
</tr>
<tr>
<td style="text-align:center">Tanh</td>
<td style="text-align:center">$$f(x) = \frac{e<sup>x-e</sup>{-x}}{e<sup>x+e</sup>{-x}}$$</td>
<td style="text-align:center">$$f'(x) = 1-(f(x))^2$$</td>
</tr>
</tbody>
</table>
<h1 id="常用激活函数图像">常用激活函数图像</h1>
<h1 id="为什么要用激活函数">为什么要用激活函数</h1>
<p>激活函数主要的作用是提供网络的非线性建模能力，如果没有激活函数，神经网络只能表达线性映射，即便有再多的隐藏层，整个网络跟单层神经网络是等价的，因为经过多层的加权计算，都可以展成一次的加权计算。<br>
何为线性，何为非线性呢？ 线性在数学上可以理解为一阶导数为常数，非线性为一阶导数不为常数。<br>
激活函数有以下性质：</p>
<ul>
<li>非线性： 首先,线性函数可以高效可靠对数据进行拟合, 但是现实生活中往往存在一些非线性的问题(如XOR), 这个时候, 我们就需要借助激活函数的非线性来对数据的分布进行重新映射, 从而获得更强大的拟合能力. (这个是最主要的原因, 其他还有下面这些性质也使得我们选择激活函数作为网络常用层)</li>
<li>可微性：因为优化方法是基于梯度的，可微性有助于我们用梯度下降的方法优化网络</li>
<li>单调性：当激活函数是单调的时候，能够保证单层网络是凸优化。</li>
<li>输出值的范围：激活函数的输出值范围可以是有限的，也可以是无限的。当是有限的时候，基于梯度的优化方法会更加稳定，因为特征的表示受有限权值的影响更加显著；当输出值是无限的时候，模型的训练会更加高效，不过在这种情况下，一般需要更小的学习率。</li>
</ul>
<h1 id="更形象的解释">更形象的解释</h1>
<h1 id="各个激活函数的比较和适用场景">各个激活函数的比较和适用场景</h1>
<p><strong>神经元饱和问题</strong>  当输入值很大或者很小时，激活函数的梯度值接近于0，在一定范围的输入梯度趋向于0，无论从深层网络传来何种梯度值，它向浅层网络中传过去的都是趋于0 的数，从而引发梯度消失的问题。<br>
<strong>zero-centered(0均值)</strong>：如果激活函数的值域不是0均值，就会导致后一层的神经元接受的输入永远为正或者为负。以 sigmod为例: $\frac{\partial L}{\partial w_{i} } = \frac{\partial L}{\partial f }\frac{\partial f}{\partial w_{i} } = \frac{\partial L}{\partial f } x_{i} f(x)(1-f(x)) $,其中的 $ f(x)(1-f(x)) $ 恒大于0, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><mi>f</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial f }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.3612159999999998em;vertical-align:-0.481108em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathdefault mtight" style="margin-right:0.10764em;">f</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.481108em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>为常数，所以<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault">x</span></span></span></span>的符号固定，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{\partial L}{\partial w_{i} }</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.325208em;vertical-align:-0.44509999999999994em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>的方向也就固定了，这样在训练时，所有weight的更新只会沿着一个方向更新（关于非0均值的问题，由于通常训练时是按batch训练的，每个batch会得到不同的信号，这在一定程度上可以缓解非0 均值的问题带来的影响，这也是ReLu虽然非0均值，但是却是主流激活函数的原因之一）。</p>
<table>
<thead>
<tr>
<th style="text-align:center">激活函数</th>
<th style="text-align:center">优势</th>
<th style="text-align:center">劣势</th>
<th style="text-align:center">适用场景</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Sigmoid</td>
<td style="text-align:center">可以将数据值域压缩到[0,1]区间内</td>
<td style="text-align:center">1.输入太大或太小，容易出现饱和 <br>2.输出的值域是非0均值的<br>3.幂运算在计算机中消耗比较大</td>
<td style="text-align:center">在logistic回归中有重要地位</td>
</tr>
<tr>
<td style="text-align:center">Tanh</td>
<td style="text-align:center">1.zero-centered:可以将<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mo>(</mo><mo>−</mo><mi mathvariant="normal">∞</mi><mo separator="true">,</mo><mo>+</mo><mi mathvariant="normal">∞</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">(-\infty,+\infty)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord">−</span><span class="mord">∞</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">+</span><span class="mord">∞</span><span class="mclose">)</span></span></span></span>的数据压缩到[-1,1]区间内<br>2.完全可微分的，反对称,对称中心在原点。</td>
<td style="text-align:center">1.神经元饱和问题，梯度消失仍然存在<br>2.计算复杂</td>
<td style="text-align:center">在分类任务中，双曲正切函数(Tanh)逐渐取代Sigmoid函数作为标准的激活函数</td>
</tr>
<tr>
<td style="text-align:center">ReLU</td>
<td style="text-align:center">1.正区间梯度始终为1，解决了正区间梯度消失的问题<br>2.计算速度加快<br>3.可以让训练过程更快收敛（实验表明比Sigmoid的收敛速度块6倍）</td>
<td style="text-align:center">1.输出是非0均值<br>2.如果输入值为负值，ReLU由于导数为0，权重无法更新，其学习速度可能会变得很慢，很容易进入Dead状态。（为了克服这个问题，在实际中，人们常常在初始化ReLU神经元时，会倾向于给他附加一个正数偏好，如0.01）</td>
<td style="text-align:center">在卷积神经网络中比较主流</td>
</tr>
<tr>
<td style="text-align:center">LeakyReLU</td>
<td style="text-align:center">1.没有神经元饱和问题<br>2.神经元不会“Dead”（因为在负值时，输出不为0，而是x的系数0.001)</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">PReLU</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">ELU</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
<tr>
<td style="text-align:center">Maxout</td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
<td style="text-align:center"></td>
</tr>
</tbody>
</table>
<h1 id="sigmoid-和-softmax的区别">Sigmoid 和 Softmax的区别</h1>
<p>sigmoid是将一个正负无穷区间的值映射到(0,1)区间，通常用作二分类问题，而softmax把一个k维的实值向量映射成一个<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mn>1</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><msub><mi>b</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">b_{1},...,b_{2},b_{k}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>,其中<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>b</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">b_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为一个0~1的常数，且它们的和为1，可以看作是属于每一类的概览，通常用做多分类问题。在二分类问题中，sigmoid和softmax是差不多的，都是求交叉熵损失函数，softmax可以看作是sigmoid的扩展，当类别为2时，根据softmax回归参数冗余的特点，可以将softmax函数推到成sigmoid函数。</p>
<p>参考链接：</p>
<h1 id="relu神经元死亡原因">ReLU神经元死亡原因</h1>
<p>在梯度下降中，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><msub><mi>w</mi><mi>i</mi></msub><mo>−</mo><mi>α</mi><mfrac><mrow><mi mathvariant="normal">∂</mi><mi>L</mi></mrow><mrow><mi mathvariant="normal">∂</mi><msub><mi>w</mi><mi>i</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">w_{i} = w_{i}-\alpha \frac{\partial L}{\partial w_{i}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.73333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.325208em;vertical-align:-0.44509999999999994em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3280857142857143em;"><span style="top:-2.357em;margin-left:-0.02691em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight" style="margin-right:0.05556em;">∂</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.44509999999999994em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，<span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \part at position 44: …w_{i}} = \frac{\̲p̲a̲r̲t̲ ̲L}{\part f} \fr…'>\frac {\partial L}{\partial w_{i}} = \frac{\part L}{\part f} \frac{\part f}{\part w_{i}}</span>,<span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \part at position 26: …}-\alpha \frac{\̲p̲a̲r̲t̲ ̲L}{\part f}x_{i…'>w_{i}=w_{i}-\alpha \frac{\part L}{\part f}x_{i}</span></p>
<p>如果<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>x</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">x_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>为正数，<span class='katex-error' title='ParseError: KaTeX parse error: Undefined control sequence: \part at position 7: \frac{\̲p̲a̲r̲t̲ ̲L}{\part f}'>\frac{\part L}{\part f}</span>为正数，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.0037em;">α</span></span></span></span>也为正数，在较大的学习率，或者异常的输入下，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w_{i}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>可能变为负数，下次前向传播为0，该神经元与下层网络之间就断开连接了，反向也断开了。</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSD]]></title>
        <id>https://boxyao.github.io/post/ssd/</id>
        <link href="https://boxyao.github.io/post/ssd/">
        </link>
        <updated>2020-04-28T11:46:09.000Z</updated>
        <content type="html"><![CDATA[<p>待更</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mask-RCNN]]></title>
        <id>https://boxyao.github.io/post/mask-rcnn/</id>
        <link href="https://boxyao.github.io/post/mask-rcnn/">
        </link>
        <updated>2020-04-28T11:45:42.000Z</updated>
        <content type="html"><![CDATA[<p>待更</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[YOLO-v4]]></title>
        <id>https://boxyao.github.io/post/yolo-v4/</id>
        <link href="https://boxyao.github.io/post/yolo-v4/">
        </link>
        <updated>2020-04-28T11:45:24.000Z</updated>
        <content type="html"><![CDATA[<p>待更</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[YOLO-v3]]></title>
        <id>https://boxyao.github.io/post/yolo-v3/</id>
        <link href="https://boxyao.github.io/post/yolo-v3/">
        </link>
        <updated>2020-04-28T11:44:54.000Z</updated>
        <content type="html"><![CDATA[<p>待更</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[YOLO-v2]]></title>
        <id>https://boxyao.github.io/post/yolo-v2/</id>
        <link href="https://boxyao.github.io/post/yolo-v2/">
        </link>
        <updated>2020-04-28T11:44:37.000Z</updated>
        <content type="html"><![CDATA[<p>待更</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[YOLO-v1]]></title>
        <id>https://boxyao.github.io/post/yolo-v1/</id>
        <link href="https://boxyao.github.io/post/yolo-v1/">
        </link>
        <updated>2020-04-28T11:44:18.000Z</updated>
        <content type="html"><![CDATA[<p>待更</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster-RCNN]]></title>
        <id>https://boxyao.github.io/post/faster-rcnn/</id>
        <link href="https://boxyao.github.io/post/faster-rcnn/">
        </link>
        <updated>2020-04-28T11:43:48.000Z</updated>
        <content type="html"><![CDATA[<p>待更</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[深度学习]]></title>
        <id>https://boxyao.github.io/post/Deep-learning/</id>
        <link href="https://boxyao.github.io/post/Deep-learning/">
        </link>
        <updated>2020-04-25T18:18:42.000Z</updated>
        <content type="html"><![CDATA[<p>😇</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://boxyao.github.io/post/hello-gridea/</id>
        <link href="https://boxyao.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T19:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>